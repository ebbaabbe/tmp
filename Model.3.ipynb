{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read simulator images from udacity training set\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "augment_cnt = 2000\n",
    "sample_cnt = int(os.popen(\"wc -l data/driving_log.csv | awk '{print $1}'\").read().strip())\n",
    "lines = np.arange(sample_cnt-1+augment_cnt)\n",
    "np.random.shuffle(lines)\n",
    "\n",
    "#80/20 split for training and validation sets\n",
    "train_split=int(len(lines)*0.8)\n",
    "train_idx=lines[0:train_split]\n",
    "valid_idx=lines[train_split:len(lines)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_img(image, corr):\n",
    "    img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY )\n",
    "    img = img[..., np.newaxis]\n",
    "    #img = img[50:300,:]\n",
    "    if corr == -1 :\n",
    "        img = np.fliplr(img)\n",
    "    return img\n",
    "\n",
    "def get_data(sample_idx, batch_size):\n",
    "    while 1:\n",
    "        df = pd.read_csv('data/driving_log.csv')\n",
    "        idx=0\n",
    "        images=[]\n",
    "        measurements = []\n",
    "        for x in range(batch_size):\n",
    "            if idx >= sample_cnt:   #flip an image\n",
    "                idx1 = idx % sample_cnt\n",
    "                corr = -1\n",
    "            else :\n",
    "                idx1 = idx\n",
    "                corr = 1\n",
    "                \n",
    "            cimg = \"data/\" + df[\"center\"][idx1].strip()\n",
    "            ang = df[\"steering\"][idx1]\n",
    "            limg = \"data/\" + df[\"left\"][idx1].strip()\n",
    "            rimg = \"data/\" + df[\"right\"][idx1].strip()\n",
    "            \n",
    "            img = cv2.imread(cimg)\n",
    "            images.append(process_img(img, corr))\n",
    "            measurements.append(ang*corr)\n",
    "            \n",
    "            img = cv2.imread(limg)\n",
    "            images.append(process_img(img, corr))\n",
    "            measurements.append((ang+0.2)*corr)\n",
    "            \n",
    "            img = cv2.imread(rimg)\n",
    "            images.append(process_img(img, corr))\n",
    "            measurements.append((ang-0.2)*corr)\n",
    "            \n",
    "            idx+=1\n",
    "        yield (np.array(images), np.array(measurements))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model based on https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/\n",
    "# Using keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dropout, Dense, Lambda, Cropping2D, Activation, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# pre process as described in lecture\n",
    "# 1. Normalize input data\n",
    "model.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(160,320,3))) \n",
    "\n",
    "# 2. Crop 60 pixels off the top and 10 pixels off the bottom of images\n",
    "model.add(Cropping2D(cropping=((60,10),(0,0)))) \n",
    "\n",
    "# Nvidia model layers\n",
    "model.add(Conv2D(filters=24, kernel_size=5, strides=(2, 2), activation='relu'))\n",
    "#model.add(BatchNormalization(axis=1))\n",
    "model.add(Conv2D(filters=36, kernel_size=5, strides=(2, 2), activation='relu'))\n",
    "model.add(Conv2D(filters=48, kernel_size=5, strides=(2, 2), activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=3, strides=(1, 1), activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=3, strides=(1, 1), activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# train model\n",
    "model.fit_generator(get_data(train_idx, 128), \n",
    "                    steps_per_epoch=len(train_idx) / 128,\n",
    "                    validation_data=get_data(valid_idx, 128),\n",
    "                    validation_steps=len(valid_idx) / 128,\n",
    "                    epochs=10)\n",
    "#model.fit_generator(, steps_per_epoch=80, epochs=5)\n",
    "#model.fit(X_train, y_train, validation_split=0.25, shuffle=True, epochs=7)\n",
    "model.save('model.nv.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display, SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# Show the model in ipython notebook\n",
    "figure = SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))\n",
    "display(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "63/62 [==============================] - 421s - loss: 0.0097 - acc: 0.2344 - val_loss: 0.0015 - val_acc: 0.2344\n"
     ]
    }
   ],
   "source": [
    "# Model based on https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/\n",
    "# Using keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dropout, Dense, Lambda, Cropping2D, Activation, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# pre process as described in lecture\n",
    "# 1. Normalize input data\n",
    "model.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(160,320,1))) \n",
    "\n",
    "# 2. Crop 60 pixels off the top and 10 pixels off the bottom of images\n",
    "model.add(Cropping2D(cropping=((60,10),(0,0)))) \n",
    "\n",
    "# Nvidia model layers\n",
    "model.add(Conv2D(filters=24, kernel_size=5, strides=(2, 2), activation='relu'))\n",
    "#model.add(BatchNormalization(axis=1))\n",
    "model.add(Conv2D(filters=36, kernel_size=5, strides=(2, 2), activation='relu'))\n",
    "model.add(Conv2D(filters=48, kernel_size=5, strides=(2, 2), activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=3, strides=(1, 1), activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(filters=64, kernel_size=3, strides=(1, 1), activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# train model\n",
    "model.fit_generator(get_data(train_idx, 128), \n",
    "                    steps_per_epoch=len(train_idx) / 128,\n",
    "                    validation_data=get_data(valid_idx, 128),\n",
    "                    validation_steps=len(valid_idx) / 128,\n",
    "                    epochs=10)\n",
    "#model.fit_generator(, steps_per_epoch=80, epochs=5)\n",
    "#model.fit(X_train, y_train, validation_split=0.25, shuffle=True, epochs=7)\n",
    "model.save('model.nv.drop.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model based on https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/\n",
    "# Using keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dropout, Dense, Lambda, Cropping2D, Activation, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# pre process as described in lecture\n",
    "# 1. Normalize input data\n",
    "model.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(160,320,1))) \n",
    "\n",
    "# 2. Crop 60 pixels off the top and 10 pixels off the bottom of images\n",
    "model.add(Cropping2D(cropping=((60,10),(0,0)))) \n",
    "\n",
    "# Nvidia model layers\n",
    "model.add(Conv2D(filters=24, kernel_size=5, strides=(2, 2), activation='relu'))\n",
    "#model.add(BatchNormalization(axis=1))\n",
    "model.add(Conv2D(filters=36, kernel_size=5, strides=(2, 2), activation='relu'))\n",
    "model.add(Conv2D(filters=48, kernel_size=5, strides=(2, 2), activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=3, strides=(1, 1), activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=3, strides=(1, 1), activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# train model\n",
    "model.fit_generator(get_data(train_idx, 128), \n",
    "                    steps_per_epoch=len(train_idx) / 128,\n",
    "                    validation_data=get_data(valid_idx, 128),\n",
    "                    validation_steps=len(valid_idx) / 128,\n",
    "                    epochs=10)\n",
    "#model.fit_generator(, steps_per_epoch=80, epochs=5)\n",
    "#model.fit(X_train, y_train, validation_split=0.25, shuffle=True, epochs=7)\n",
    "model.save('model.nv.2.drop.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrasquinha/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), strides=(2, 2), padding=\"valid\")`\n",
      "/home/mrasquinha/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:16: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), padding=\"same\", strides=None)`\n",
      "/home/mrasquinha/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:19: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=3, momentum=0.9, epsilon=0.001)`\n",
      "/home/mrasquinha/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), padding=\"same\")`\n",
      "/home/mrasquinha/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (7, 1), padding=\"same\")`\n",
      "/home/mrasquinha/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:25: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 7), padding=\"same\")`\n",
      "/home/mrasquinha/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:26: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=3, momentum=0.9, epsilon=0.001)`\n",
      "/home/mrasquinha/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:29: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/mrasquinha/anaconda3/lib/python3.5/site-packages/keras/legacy/layers.py:460: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/home/mrasquinha/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), strides=(2, 2), activation=\"elu\", padding=\"valid\")`\n",
      "/home/mrasquinha/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:32: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), strides=(2, 2), activation=\"elu\", padding=\"valid\")`\n",
      "/home/mrasquinha/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:33: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=-1, momentum=0.9, epsilon=0.001)`\n",
      "/home/mrasquinha/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:42: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "26/62 [===========>..................] - ETA: 182s - loss: 20.3716"
     ]
    }
   ],
   "source": [
    "# Build a model\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout, Lambda\n",
    "from keras.layers import Input\n",
    "from keras.activations import relu, softmax\n",
    "from keras.layers.convolutional import Convolution2D, Cropping2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import merge\n",
    "\n",
    "model_input = Input(shape=(160, 320, 1))\n",
    "model_stem = model_input\n",
    "model_stem = Lambda(lambda x: (x-128.0)/255.0, input_shape=model_stem.get_shape()[1:]) (model_stem)\n",
    "model_stem = Cropping2D(cropping=((60,20),(0,0)), input_shape=model_stem.get_shape()[1:]) (model_stem)\n",
    "model_stem = Convolution2D(16, 3, 3, border_mode='valid', subsample=(2, 2))(model_stem)\n",
    "model_stem = MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='same') (model_stem)\n",
    "branch_input = Activation('elu')(model_stem)\n",
    "\n",
    "branch_input = BatchNormalization(epsilon=0.001, mode=0, axis=3, momentum=0.9) (branch_input)\n",
    "\n",
    "left_branch = Convolution2D(32, 1 , 1,border_mode='same')(branch_input)\n",
    "\n",
    "right_branch = Convolution2D(32, 7, 1, border_mode='same')  (branch_input)\n",
    "right_branch = Activation('elu') (right_branch)\n",
    "right_branch = Convolution2D(32, 1, 7, border_mode='same') (right_branch)\n",
    "right_branch = BatchNormalization(epsilon=0.001, mode=0, axis=3, momentum=0.9) (right_branch)\n",
    "right_branch = Activation('elu') (right_branch)\n",
    "\n",
    "merged = merge([left_branch, right_branch], mode='sum') \n",
    "\n",
    "merged = Convolution2D(32, 3, 3, border_mode='valid', activation='elu', subsample=(2, 2))(merged)\n",
    "merged = Convolution2D(64, 3, 3, border_mode='valid', activation='elu',  subsample=(2, 2))(merged)\n",
    "merged = BatchNormalization(epsilon=0.001, mode=0, axis=-1, momentum=0.9)(merged)\n",
    "merged = Flatten()(merged)\n",
    "merged = Dropout(0.5)(merged)\n",
    "merged = Dense(128, activation='elu')(merged)\n",
    "merged = Dense(48)(merged)\n",
    "merged = Dense(1)(merged)\n",
    "\n",
    "\n",
    "# Compile and train the model\n",
    "model =  Model(input=model_input, output=merged) \n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# train model\n",
    "history = model.fit_generator(get_data(train_idx, 128), \n",
    "                    steps_per_epoch=len(train_idx) / 128,\n",
    "                    validation_data=get_data(valid_idx, 128),\n",
    "                    validation_steps=len(valid_idx) / 128,\n",
    "                    epochs=1)\n",
    "#model.fit_generator(, steps_per_epoch=80, epochs=5)\n",
    "#model.fit(X_train, y_train, validation_split=0.25, shuffle=True, epochs=7)\n",
    "model.save('model.nv.2.drop.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Behavioral Cloning Project**\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "* Use the simulator to collect data of good driving behavior\n",
    "* Build, a convolution neural network in Keras that predicts steering angles from images\n",
    "* Train and validate the model with a training and validation set\n",
    "* Test that the model successfully drives around track one without leaving the road\n",
    "* Summarize the results with a written report\n",
    "\n",
    "\n",
    "#### Files Submitted & Code Quality\n",
    "\n",
    "*1. Submission includes all required files and can be used to run the simulator in autonomous mode*\n",
    "\n",
    "My project includes the following files:\n",
    "* Model.ipynb containing the script to create and train the model\n",
    "* drive.py for driving the car in autonomous mode; Changed the speed to 15 from the default udacity code\n",
    "* model.h5 containing a trained convolution neural network\n",
    "* writeup_report.md or writeup_report.pdf summarizing the results\n",
    "\n",
    "*2. Submission includes functional code*\n",
    "Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing\n",
    "```sh\n",
    "python drive.py model.h5\n",
    "```\n",
    "\n",
    "*3. Submission code is usable and readable*\n",
    "The Model.ipynb file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model, and it contains comments to explain how the code works.\n",
    "\n",
    "#### Model Architecture and Training Strategy\n",
    "\n",
    "*1. An appropriate model architecture has been employed*\n",
    "\n",
    "My model is a slightly modified version of the Nvidia architecture described in class (http://images.nvidia.com/content/tegra/automotive/images/ 2016/solutions/pdf/end-to-end-dl-using-px.pdf).\n",
    "Figure 4 and \"section 4 Network Architecture\" provided all the details for the base implementation. I expermiented with different max pooloing   and dropout  layers. Finally settled on a single additional batch normalization layer after the first Convolution layer.\n",
    "\n",
    "I also used the cropping layer + normalization layer as described in the lectures. This is done before the convolution layers.\n",
    "\n",
    "*2. Attempts to reduce overfitting in the model*\n",
    "\n",
    "Experimented with dropout layers. The best accuracy was with the additional batch norm layer.\n",
    "\n",
    "The model was trained and validated on different data sets to ensure that the model was not overfitting. The model was tested by running it through the simulator and ensuring that the vehicle could stay on the track.\n",
    "\n",
    "*3. Model parameter tuning*\n",
    "\n",
    "The model used an adam optimizer, so the learning rate was not tuned manually.\n",
    "\n",
    "*4. Appropriate training data*\n",
    "\n",
    "I eventually used the images provided. Center+left+right camera images. I flipped some randomly picked images as can be seen in the graph after  cnt 8000. I set the validation split to be 25%. I also used some images from training the model using the simulator. I have appended the images I have generated to the end of the driving_log.csv. The simulator does not have a good response on my laptop and hence you can see the much bigger steering angles in the plot for x_val > 8000. \n",
    "\n",
    "I converted the images to RGB as a preprocessing step. I have also tried to use cv2 on a second set of images to visualize the effects. I was not sure how to get intermediate outputs from keras.\n",
    "\n",
    "#### Model Architecture and Training Strategy\n",
    "\n",
    "*1. Solution Design Approach*\n",
    "1. Try to collect training images using simulator\n",
    "2. Used udacity training data\n",
    "3. Crop and normalize layers using keras\n",
    "4. Nvidia deep learning network model\n",
    "5. Experiment with different batch norm/max pooling/drop out layers. Captured model.h5 files and moitored validation accuracy.\n",
    "6. Tweaked the steering angle in the original training data and tried the network.\n",
    "7. Picked the best model that runs on the simulator in auto mode\n",
    "\n",
    "*2. Final Model Architecture*\n",
    "Described above\n",
    "\n",
    "*3. Creation of the Training Set & Training Process*\n",
    "Collecting my own images did not work very well. I have used a few of them for training the network.\n",
    "\n",
    "I tried the use the fit generator aproach after reading this really amazing quora answer.\n",
    "https://www.quora.com/Intuitively-how-does-mini-batch-size-affect-the-performance-of-stochastic-gradient-descent\n",
    "However the smaller batch size, greater number of epochs was taking too long to run and did not yeild a higer accuracy in reasonable time. \n",
    "Thankfully while using the entire X_train set, I did not run into memory issues and this approach seemed to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
